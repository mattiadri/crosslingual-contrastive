\section{Representation Analysis}
\input{sections/tables_diagnostics}

While retrieval metrics quantify the effectiveness of multilingual alignment, they do not reveal how alignment reshapes the underlying representation space. We therefore conduct an in-depth analysis of text embeddings before and after image-pivot alignment, focusing on representational similarity, geometric properties, and feature statistics. All analyses are performed separately per language and summarized using macro averages.

\subsubsection{Cross-Lingual Representational Similarity}

To measure structural alignment across languages, we perform Representational Similarity Analysis (RSA) by computing correlations between Gram matrices of text embeddings across languages. Specifically, for each language we compute the cosine similarity matrix over a shared set of samples, and then measure Pearson correlation between the flattened upper triangles of these matrices across language pairs.

Gram matrices capture relational structure up to orthogonal transformations, making them a natural tool to compare embedding geometries across languages.

After image-pivot alignment, cross-lingual representational similarity increases under the linear residual head, while the MLP head improves retrieval without necessarily increasing RSA (Table~\ref{tab:diag_holdout_macro}). On the holdout folds, the mean off-diagonal Gram correlation increases from 0.345 $\pm$ 0.021 to 0.350 $\pm$ 0.022 under the linear head, while remaining stable under the MLP head. These gains indicate that multilingual alignment manifests not merely as instance-level retrieval improvements, but as a deeper convergence in the relational structure of the embedding space.

This divergence between retrieval improvements and RSA gains highlights that stronger downstream performance does not necessarily imply deeper convergence of representational geometry. Importantly, this increase is observed across most language pairs rather than being driven by a small subset of dominant languages, suggesting that alignment emerges globally rather than through partial collapse.

\subsubsection{Intrinsic Dimensionality and Effective Rank}

We next analyze how image-pivot alignment affects the intrinsic dimensionality of text embeddings. We measure effective rank, defined as the exponential of the entropy of the singular value spectrum, as well as the number of principal components required to explain 90\% of variance.

Across languages, image-pivot alignment leads to a moderate reduction in effective rank on the holdout folds (from 112.24 $\pm$ 1.71 to 107.93 $\pm$ 1.74 under the linear head), while the MLP head largely preserves the original dimensionality. Similarly, the number of components required to explain 90\% of variance decreases by approximately 4--5 components on average.

This behavior suggests that alignment induces a controlled compression of the representation space, removing redundant or language-specific variation while preserving semantic capacity. Crucially, the reduction in dimensionality is gradual and consistent across languages, rather than abrupt, which would be indicative of representational collapse.

Notably, this controlled reduction in effective rank is primarily observed under the linear head, while the MLP largely preserves the original dimensionality, consistent with its weaker impact on global structural alignment.

\subsubsection{Isotropy and Hubness}

To assess the global geometry of the embedding space, we compute mean pairwise cosine similarity among text embeddings as a proxy for isotropy. High mean similarity values indicate anisotropic spaces dominated by a small number of directions, which are known to harm retrieval performance.

After alignment, mean pairwise cosine similarity decreases substantially on the holdout folds (from 0.660 $\pm$ 0.004 to 0.625 $\pm$ 0.011 under the linear head), indicating improved isotropy. Direct hubness measurements based on $k$-NN in-degree statistics remain largely stable (Table~\ref{tab:diag_holdout_macro}), suggesting that image-pivot alignment improves global geometry without inducing additional hub dominance.

This geometric improvement is consistent with the observed retrieval gains, supporting the hypothesis that improved isotropy---rather than increased hubness---contributes to better cross-lingual alignment.

\subsubsection{Local Micro-Corrections (Qualitative UMAP)}

Global 2D projections of the full embedding space are visually uninformative in our setting because the learned alignment heads apply small, local updates. To make these micro-corrections visible, we visualize local neighborhoods around captions that exhibit the largest increase in cosine similarity to their paired image after alignment.

For each anchor caption (English pivot), we fit UMAP on a local set containing neighboring images and the corresponding text embeddings (before, after-linear, after-MLP), and overlay the anchor displacement. This visualization is qualitative and is not used to derive quantitative claims.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/umap_local_pivot_en_lin_vs_mlp.png}
    \caption{Local UMAP visualization of image--text alignment micro-corrections (fold~0).
    For each panel, we select an English anchor caption and visualize a local neighborhood of images (black $\times$) and text embeddings.
    Colored circles, squares, and triangles denote the anchor caption before alignment, after linear alignment, and after MLP alignment, respectively.
    Arrows indicate the displacement from the pretrained embedding (before) to the representations obtained after linear and MLP-based image-pivot alignment (arrows are scaled for visibility).
    The gold star marks the paired image associated with the anchor caption.
    The figure illustrates that alignment emerges through small, structured local corrections rather than global rearrangements of the embedding space.}
    \label{fig:umap_micro_corrections}
\end{figure}

\subsubsection{Feature Activation Statistics}

Finally, we examine low-level feature statistics to detect potential degeneration of representations. We measure the percentage of near-zero activations (PoZ) and the entropy of unit activations for each embedding dimension.

PoZ is computed as the fraction of activations with $|x| < 10^{-3}$; entropy is computed via per-dimension histograms with 30 bins over $[-1,1]$.

Across all languages, PoZ slightly decreases on the holdout folds (from 0.0363 $\pm$ 0.0003 to 0.0358 $\pm$ 0.0004 under the linear head), while average entropy increases (from 0.745 $\pm$ 0.002 to 0.777 $\pm$ 0.007 under the linear head). These trends indicate healthier representations with more informative and evenly utilized dimensions.

\subsubsection{Language-ID Probe}

To assess how much language-identifying information is retained after alignment, we train a linear language-ID probe (multinomial logistic regression) to predict the caption language from frozen text embeddings. Probe accuracy remains close to chance and largely unchanged after image-pivot alignment (Table~\ref{tab:diag_holdout_macro}), suggesting that visual grounding improves multilingual structure without aggressively erasing language-specific signals.

\subsubsection{Summary}

Overall, image-pivoted contrastive learning induces multilingual alignment through structured geometric convergence when using a linear residual head, while nonlinear MLP heads primarily improve retrieval performance through increased capacity. The combined diagnostics indicate that alignment improves isotropy and cross-lingual structure without inducing representational collapse, excessive hubness, or strong language invariance. These results underscore the importance of complementing downstream metrics with representation-level diagnostics when evaluating multilingual multimodal alignment.